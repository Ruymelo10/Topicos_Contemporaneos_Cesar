{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"uMYs5giiErH1","executionInfo":{"status":"ok","timestamp":1724111870136,"user_tz":180,"elapsed":8270,"user":{"displayName":"RUY OVIDIO PERRELLI DE MELO","userId":"11031392736059221395"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, Subset\n","import random\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"9vbViM5dErH2","executionInfo":{"status":"ok","timestamp":1724111870136,"user_tz":180,"elapsed":4,"user":{"displayName":"RUY OVIDIO PERRELLI DE MELO","userId":"11031392736059221395"}}},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"HVMqKzd3ErH2","executionInfo":{"status":"ok","timestamp":1724111870136,"user_tz":180,"elapsed":3,"user":{"displayName":"RUY OVIDIO PERRELLI DE MELO","userId":"11031392736059221395"}}},"outputs":[],"source":["num_letters = 5\n","\n","# Cria um dicionário que mapeia pares de letras como \"AB\", \"DE\" para inteiros únicos\n","tok2idx = {f\"{chr(65 + i)}{chr(65 + j)}\": i * num_letters + j for i in range(num_letters) for j in range(num_letters)}\n","\n","# Tokeniza um texto\n","def tokenizer(text):\n","    words = text.split()\n","    output = torch.tensor([tok2idx[word] for word in words], dtype=torch.long)\n","    return output\n","\n","# Função para converter um tensor de inteiros em texto\n","def get_text_from_tensor(tensor):\n","    return ' '.join([list(tok2idx.keys())[i] for i in tensor])"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ubPnqLCKErH2","executionInfo":{"status":"ok","timestamp":1724111870136,"user_tz":180,"elapsed":3,"user":{"displayName":"RUY OVIDIO PERRELLI DE MELO","userId":"11031392736059221395"}}},"outputs":[],"source":["# Gera exemplos de treinamento\n","def generate_examples(N):\n","    examples = []\n","    max_letter = 65 + num_letters - 1\n","    for _ in range(N):\n","        # Gera dois pares de letras aleatórios\n","        first_pair = f\"{chr(random.randint(65, max_letter))}{chr(random.randint(65, max_letter))}\"  # Random pair like \"AB\"\n","        second_pair = f\"{chr(random.randint(65, max_letter))}{chr(random.randint(65, max_letter))}\"  # Random pair like \"DE\"\n","\n","        # Gera o target que é a primeira letra do primeiro par e a segunda letra do segundo par\n","        target = f\"{first_pair[0]}{second_pair[1]}\"\n","\n","        # Adiciona o exemplo à lista\n","        examples.append((first_pair, second_pair, target))\n","\n","    return examples"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"ifa8qnwkErH3","executionInfo":{"status":"ok","timestamp":1724111870136,"user_tz":180,"elapsed":3,"user":{"displayName":"RUY OVIDIO PERRELLI DE MELO","userId":"11031392736059221395"}}},"outputs":[],"source":["N = 5000\n","examples = generate_examples(N)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"uBXR7_FUErH3","executionInfo":{"status":"ok","timestamp":1724111870137,"user_tz":180,"elapsed":3,"user":{"displayName":"RUY OVIDIO PERRELLI DE MELO","userId":"11031392736059221395"}}},"outputs":[],"source":["# Cria um dataset com os exemplos\n","class SimpleTokenDataset(Dataset):\n","    def __init__(self, examples):\n","        self.data = examples\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        input_seq1, input_seq2, target_seq = self.data[idx]\n","        src = tokenizer(input_seq1 + \" \" + input_seq2)\n","        tgt = tokenizer(target_seq).squeeze()\n","        return src, tgt"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"1he-cuf_ErH3","executionInfo":{"status":"ok","timestamp":1724111870137,"user_tz":180,"elapsed":3,"user":{"displayName":"RUY OVIDIO PERRELLI DE MELO","userId":"11031392736059221395"}}},"outputs":[],"source":["dataset = SimpleTokenDataset(examples)\n","\n","num_val = 0.2 * N\n","\n","train_set = Subset(dataset, range(N - int(num_val)))\n","val_set = Subset(dataset, range(N - int(num_val), N))\n","\n","train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n","val_loader = DataLoader(val_set, batch_size=16)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"YG2BCqxFErH3","executionInfo":{"status":"ok","timestamp":1724111870634,"user_tz":180,"elapsed":500,"user":{"displayName":"RUY OVIDIO PERRELLI DE MELO","userId":"11031392736059221395"}}},"outputs":[],"source":["class TextGenerator(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","\n","        # Apenas o último hidden state é utilizado\n","        _, (h_n, _) = self.lstm(x)\n","\n","        out = self.fc(h_n.squeeze(0))\n","        return out\n","\n","\n","vocab_size = 10 * 10\n","embedding_dim = 128\n","hidden_dim = 256\n","output_dim = vocab_size\n","\n","model = TextGenerator(vocab_size, embedding_dim, hidden_dim, output_dim)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"nW9UQXRFErH3","executionInfo":{"status":"ok","timestamp":1724111873813,"user_tz":180,"elapsed":3180,"user":{"displayName":"RUY OVIDIO PERRELLI DE MELO","userId":"11031392736059221395"}}},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"fPI-lXZwErH3","executionInfo":{"status":"ok","timestamp":1724111886148,"user_tz":180,"elapsed":12341,"user":{"displayName":"RUY OVIDIO PERRELLI DE MELO","userId":"11031392736059221395"}},"outputId":"bf4dcaeb-7e08-406e-fd43-ddb972bc79c6","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stderr","text":["Train Epoch 1/5: 100%|██████████| 250/250 [00:02<00:00, 94.43it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 1.0229\n","Val Loss: 0.0281\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 2/5: 100%|██████████| 250/250 [00:02<00:00, 90.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.0145\n","Val Loss: 0.0080\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 3/5: 100%|██████████| 250/250 [00:02<00:00, 102.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.0055\n","Val Loss: 0.0039\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 4/5: 100%|██████████| 250/250 [00:02<00:00, 112.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.0030\n","Val Loss: 0.0024\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 5/5: 100%|██████████| 250/250 [00:01<00:00, 166.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train Loss: 0.0019\n","Val Loss: 0.0016\n"]}],"source":["epochs = 5\n","for epoch in range(epochs):\n","    # Treinamento\n","    model.train()\n","    train_loss = 0\n","    for input_seq, target_seq in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}/{epochs}\"):\n","        optimizer.zero_grad()\n","        output = model(input_seq)\n","        loss = criterion(output, target_seq)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()\n","\n","    avg_train_loss = train_loss / len(train_loader)\n","    print(f'Train Loss: {avg_train_loss:.4f}')\n","\n","    # Validação\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for input_seq, target_seq in val_loader:\n","            output = model(input_seq)\n","            loss = criterion(output, target_seq)\n","            val_loss += loss.item()\n","\n","    avg_val_loss = val_loss / len(val_loader)\n","    print(f'Val Loss: {avg_val_loss:.4f}')"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"rzeEkUAfErH3","executionInfo":{"status":"ok","timestamp":1724111886148,"user_tz":180,"elapsed":17,"user":{"displayName":"RUY OVIDIO PERRELLI DE MELO","userId":"11031392736059221395"}},"outputId":"04c9a4cb-8217-4f47-a89c-cc01845dd58e","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Input: DE AE, Predicted: DE\n"]}],"source":["# Inferência\n","input_text = \"DE AE\"\n","inputs = tokenizer(input_text)\n","\n","output = model(inputs)\n","predicted_token = output.argmax().unsqueeze(0)\n","predicted_text = get_text_from_tensor(predicted_token)\n","\n","print(f'Input: {input_text}, Predicted: {predicted_text}')"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"SDesQoP9ErH3","executionInfo":{"status":"ok","timestamp":1724111886148,"user_tz":180,"elapsed":14,"user":{"displayName":"RUY OVIDIO PERRELLI DE MELO","userId":"11031392736059221395"}},"outputId":"0224bed0-f91a-4b3e-f2c2-4ce6b554c328","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Input: AB CD, Predicted: AD CD AD CD AD\n"]}],"source":["def generate_autoregressive_text(model, input_text, seq_len):\n","    inputs = tokenizer(input_text)\n","    outputs = []\n","\n","    for _ in range(seq_len):\n","        output = model(inputs)\n","        predicted_token = output.argmax().unsqueeze(0)\n","        inputs = torch.cat([inputs[-1].unsqueeze(0), predicted_token])\n","        outputs.append(predicted_token)\n","\n","    return get_text_from_tensor(outputs)\n","\n","input_text = \"AB CD\"\n","seq_len = 5\n","predicted_text = generate_autoregressive_text(model, input_text, seq_len)\n","\n","print(f'Input: {input_text}, Predicted: {predicted_text}')"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[{"file_id":"https://github.com/silvaan/topicos_contemporaneos/blob/main/Part%202%20-%20LLMs/03%20-%20Language%20Model.ipynb","timestamp":1724111831723}]}},"nbformat":4,"nbformat_minor":0}